setwd("~/Documents/GitHub/Spring2018/Project_Starter_Codes/Project2-PredictiveModelling/doc")
knitr::opts_chunk$set(echo = TRUE)
library("EBImage")
library("gbm")
img_train_dir  <- "../../../../Proj2_Data/train/" # This is where my data lives (outside of Spring2018)
n_files <- length(list.files(img_train_dir))
n_files
list.files(img_train_dir)
length(list.files(img_train_dir))
n_files <- length(list.files(img_train_dir))
n_files
dat <- matrix(NA, nrow = n_files, ncol = 3)
for(i in 1:n_files){
img <- readImage(paste0(img_train_dir,  "pet", i, ".jpg"))
dat[i, 1:length(dim(img))] <- dim(img)
}
# How many B/W images?  All color.
table(dat[, 3])
# How many rows? A lot at 500 rows.  Maybe a good subset to consider.
table(dat[, 1])
cv.function <- function(X.train, y.train, d, K) {
n        <- length(y.train)
n.fold   <- floor(n/K)
s        <- sample(rep(1:K, c(rep(n.fold, K-1), n-(K-1)*n.fold)))
cv.error <- rep(NA, K)
for (i in 1:K){
train.data  <- X.train[s != i,]
train.label <- y.train[s != i]
test.data   <- X.train[s == i,]
test.label  <- y.train[s == i]
par  <- list(depth = d)
fit  <- train(train.data, train.label, par)
pred <- test(fit, test.data)
cv.error[i] <- mean(pred != test.label)
}
return(c(mean(cv.error), sd(cv.error)))
}
model_values <- seq(3, 11, 2)
model_labels <- paste("GBM with depth =", model_values)
la
lab
n_r    <- 500
subset <- which(dat[,1] == n_r)
### store vectorized pixel values of images
dat <- matrix(NA, length(subset), n_r)
for(i in subset){
img     <- readImage(paste0(img_train_dir,  "pet", i, ".jpg"))
dat[i,] <- rowMeans(img)
}
i
rowMeans(img)
n_r    <- 500
subset <- which(dat[,1] == n_r)
### store vectorized pixel values of images
dat <- matrix(NA, length(subset), n_r)
row <- 1
for(i in subset){
img     <- readImage(paste0(img_train_dir,  "pet", i, ".jpg"))
dat[row,] <- rowMeans(img)
row <- row + 1
}
label_train <- read.csv("../../../../Proj2_Data/train_label.txt", header = F)
label_train <- read.csv("../../../../Proj2_Data/train_label.txt", header = F)
label_train <- as.numeric(unlist(label_train) == "cat")
label_train
label_train <- label_train[subset]
dim(da)
dim(dat)
n_r    <- 500
subset <- which(dat[,1] == n_r)
### store vectorized pixel values of images
dat <- matrix(NA, length(subset), n_r)
row <- 1
dat
dim(dat)
n_files <- length(list.files(img_train_dir))
### store image dimensions
dat <- matrix(NA, nrow = n_files, ncol = 3)
for(i in 1:n_files){
img <- readImage(paste0(img_train_dir,  "pet", i, ".jpg"))
dat[i, 1:length(dim(img))] <- dim(img)
}
# How many B/W images?  All color.
table(dat[, 3])
# How many rows? A lot at 500 rows.  Maybe a good subset to consider.
table(dat[, 1])
n_r    <- 500
subset <- which(dat[,1] == n_r)
### store vectorized pixel values of images
dat <- matrix(NA, length(subset), n_r)
row <- 1
dim(dat)
for(i in subset){
img     <- readImage(paste0(img_train_dir,  "pet", i, ".jpg"))
dat[row,] <- rowMeans(img)
row <- row + 1
}
model_values <- seq(3, 11, 2)
model_labels <- paste("GBM with depth =", model_values)
label_train <- read.csv("../../../../Proj2_Data/train_label.txt", header = F)
label_train <- as.numeric(unlist(label_train) == "cat")
label_train <- label_train[subset]
length(label_train)
err_cv <- array(dim = c(length(model_values), 2))
for(k in 1:length(model_values)){
cat("k=", k, "\n")
err_cv[k,] <- cv.function(dat, label_train, model_values[k], K)
}
K <- 5
err_cv <- array(dim = c(length(model_values), 2))
for(k in 1:length(model_values)){
cat("k=", k, "\n")
err_cv[k,] <- cv.function(dat, label_train, model_values[k], K)
}
K <- 5
err_cv <- array(dim = c(length(model_values), 2))
for(k in 1:length(model_values)){
cat("k=", k, "\n")
err_cv[k,] <- cv.function(dat, label_train, model_values[k], K)
}
train <- function(dat_train, label_train, par = NULL){
### Train a Gradient Boosting Model (GBM) using processed features from training images
### Input:
###  -  processed features from images
###  -  class labels for training images
### Output: training model specification
### load libraries
library("gbm")
### Train with gradient boosting model
if(is.null(par)){
depth <- 3
} else {
depth <- par$depth
}
fit_gbm <- gbm.fit(x = dat_train, y = label_train,
n.trees = 2000,
distribution = "bernoulli",
interaction.depth = depth,
bag.fraction = 0.5,
verbose = FALSE)
best_iter <- gbm.perf(fit_gbm, method = "OOB", plot.it = FALSE)
return(list(fit = fit_gbm, iter = best_iter))
}
K <- 5
err_cv <- array(dim = c(length(model_values), 2))
for(k in 1:length(model_values)){
cat("k=", k, "\n")
err_cv[k,] <- cv.function(dat, label_train, model_values[k], K)
}
source("../lib/cross_validation.R")
source("../lib/train.R")
source("../lib/test.R")
model_values <- seq(3, 11, 2)
model_labels <- paste("GBM with depth =", model_values)
label_train <- read.csv("../../../../Proj2_Data/train_label.txt", header = F)
label_train <- as.numeric(unlist(label_train) == "cat")
label_train <- label_train[subset]
K <- 5
err_cv <- array(dim = c(length(model_values), 2))
for(k in 1:length(model_values)){
cat("k=", k, "\n")
err_cv[k,] <- cv.function(dat, label_train, model_values[k], K)
}
warnings()
plot(model_values, err_cv[,1], xlab = "Interaction Depth", ylab = "CV Error",
main = "Cross Validation Error", type = "n", ylim = c(0, 0.25))
points(model_values, err_cv[,1], col = "blue", pch=16)
lines(model_values, err_cv[,1], col = "blue")
arrows(model_values, err_cv[,1] - err_cv[,2], model_values, err_cv[,1] + err_cv[,2],
length = 0.1, angle = 90, code = 3)
err_cv[,1]
err_cv
model_best <- model_values[1]
if(run.cv){
model_best <- model_values[which.min(err_cv[, 1])]
}
par_best <- list(depth = model_best)
model_best <- model_values[which.min(err_cv[, 1])]
par_best <- list(depth = model_best)
par_best
tm_train <- NA
tm_train <- system.time(fit_train <- train(dat_train, label_train, par_best))
tm_train <- NA
tm_train <- system.time(fit_train <- train(dat, label_train, par_best))
