facet_wrap(~ topic, scales = "free", ncol = 4) +
coord_flip()
?png
packages.used <- c("ggplot2", "dplyr", "tibble", "tidyr",  "stringr", "tidytext", "topicmodels", "wordcloud", "ggridges")
# check packages that need to be installed.
packages.needed <- setdiff(packages.used, intersect(installed.packages()[,1], packages.used))
# install additional packages
if(length(packages.needed) > 0) {
install.packages(packages.needed, dependencies = TRUE, repos = 'http://cran.us.r-project.org')
}
library(ggplot2)
library(dplyr)
library(tibble)
library(tidyr)
library(stringr)
library(tidytext)
library(topicmodels)
library(wordcloud)
library(ggridges)
source("../libs/multiplot.R")
spooky <- read.csv('../data/spooky.csv', as.is = TRUE)
head(spooky)
summary(spooky)
sum(is.na(spooky))
spooky$author <- as.factor(spooky$author)
# Make a table with one word per row and remove `stop words` (i.e. the common words).
spooky_wrd <- unnest_tokens(spooky, word, text)
spooky_wrd <- anti_join(spooky_wrd, stop_words, by = "word")
# Words is a list of words, and freqs their frequencies
words <- count(group_by(spooky_wrd, word))$word
freqs <- count(group_by(spooky_wrd, word))$n
head(sort(freqs, decreasing = TRUE))
#png("../figs/Wordcloud_all.png")
wordcloud(words, freqs, max.words = 50, color = c("purple4", "red4", "black"))
#dev.off()
# Counts number of times each author used each word.
author_words <- count(group_by(spooky_wrd, word, author))
# Counts number of times each word was used.
all_words    <- rename(count(group_by(spooky_wrd, word)), all = n)
author_words <- left_join(author_words, all_words, by = "word")
author_words <- arrange(author_words, desc(all))
author_words <- ungroup(head(author_words, 81))
ggplot(author_words) +
geom_col(aes(reorder(word, all, FUN = min), n, fill = author)) +
xlab(NULL) +
coord_flip() +
facet_wrap(~ author) +
theme(legend.position = "none")
p1 <- ggplot(spooky) +
geom_bar(aes(author, fill = author)) +
theme(legend.position = "none")
spooky$sen_length <- str_length(spooky$text)
head(spooky$sen_length)
p2 <- ggplot(spooky) +
geom_density_ridges(aes(sen_length, author, fill = author)) +
scale_x_log10() +
theme(legend.position = "none") +
labs(x = "Sentence length [# characters]")
spooky_wrd$word_length <- str_length(spooky_wrd$word)
head(spooky_wrd$word_length)
p3 <- ggplot(spooky_wrd) +
geom_density(aes(word_length, fill = author), bw = 0.05, alpha = 0.3) +
scale_x_log10() +
theme(legend.position = "none") +
labs(x = "Word length [# characters]")
layout <- matrix(c(1, 2, 1, 3), 2, 2, byrow = TRUE)
multiplot(p1, p2, p3, layout = layout)
frequency <- count(spooky_wrd, author, word)
tf_idf    <- bind_tf_idf(frequency, word, author, n)
head(tf_idf)
tail(tf_idf)
tf_idf    <- arrange(tf_idf, desc(tf_idf))
tf_idf    <- mutate(tf_idf, word = factor(word, levels = rev(unique(word))))
# Grab the top thirty tf_idf scores in all the words
tf_idf_30 <- top_n(tf_idf, 30, tf_idf)
ggplot(tf_idf_30) +
geom_col(aes(word, tf_idf, fill = author)) +
labs(x = NULL, y = "TF-IDF values") +
theme(legend.position = "top", axis.text.x  = element_text(angle=45, hjust=1, vjust=0.9))
# Grab the top twenty tf_idf scores in all the words for each author
tf_idf <- ungroup(top_n(group_by(tf_idf, author), 20, tf_idf))
ggplot(tf_idf) +
geom_col(aes(word, tf_idf, fill = author)) +
labs(x = NULL, y = "tf-idf") +
theme(legend.position = "none") +
facet_wrap(~ author, ncol = 3, scales = "free") +
coord_flip() +
labs(y = "TF-IDF values")
# Keep words that have been classified within the NRC lexicon.
get_sentiments('nrc')
sentiments <- inner_join(spooky_wrd, get_sentiments('nrc'), by = "word")
count(sentiments, sentiment)
count(sentiments, author, sentiment)
ggplot(count(sentiments, sentiment)) +
geom_col(aes(sentiment, n, fill = sentiment))
ggplot(count(sentiments, author, sentiment)) +
geom_col(aes(sentiment, n, fill = sentiment)) +
facet_wrap(~ author) +
coord_flip() +
theme(legend.position = "none")
nrc_pos <- filter(get_sentiments('nrc'), sentiment == "positive")
nrc_pos
positive <- inner_join(spooky_wrd, nrc_pos, by = "word")
head(positive)
count(positive, word, sort = TRUE)
pos_words     <- count(group_by(positive, word, author))
pos_words_all <- count(group_by(positive, word))
pos_words <- left_join(pos_words, pos_words_all, by = "word")
pos_words <- arrange(pos_words, desc(n.y))
pos_words <- ungroup(head(pos_words, 81))
# Note the above is the same as
# pos_words <- pos_words  %>%
#                left_join(pos_words_all, by = "word") %>%
#                arrange(desc(n.y)) %>%
#                head(81) %>%
#                ungroup()
ggplot(pos_words) +
geom_col(aes(reorder(word, n.y, FUN = min), n.x, fill = author)) +
xlab(NULL) +
coord_flip() +
facet_wrap(~ author) +
theme(legend.position = "none")
# Counts how many times each word appears in each sentence
sent_wrd_freqs <- count(spooky_wrd, id, word)
head(sent_wrd_freqs)
# Creates a DTM matrix
spooky_wrd_tm <- cast_dtm(sent_wrd_freqs, id, word, n)
spooky_wrd_tm
length(unique(spooky_wrd$id))
length(unique(spooky_wrd$word))
spooky_wrd_lda    <- LDA(spooky_wrd_tm, k = 12, control = list(seed = 1234))
spooky_wrd_topics <- tidy(spooky_wrd_lda, matrix = "beta")
spooky_wrd_topics
# Grab the top five words for each topic.
spooky_wrd_topics_5 <- ungroup(top_n(group_by(spooky_wrd_topics, topic), 5, beta))
spooky_wrd_topics_5 <- arrange(spooky_wrd_topics_5, topic, -beta)
spooky_wrd_topics_5 <- mutate(spooky_wrd_topics_5, term = reorder(term, beta))
ggplot(spooky_wrd_topics_5) +
geom_col(aes(term, beta, fill = factor(topic)), show.legend = FALSE) +
facet_wrap(~ topic, scales = "free", ncol = 4) +
coord_flip()
spooky_wrd_topics <- mutate(spooky_wrd_topics, topic = paste0("topic", topic))
spooky_wrd_topics <- spread(spooky_wrd_topics, topic, beta)
spooky_wrd_topics_12 <- filter(spooky_wrd_topics, topic2 > .001 | topic3 > .001)
spooky_wrd_topics_12 <- mutate(spooky_wrd_topics_12, log_ratio = log10(topic2 / topic1))
spooky_wrd_topics_12 <- group_by(spooky_wrd_topics_12, direction = log_ratio > 0)
spooky_wrd_topics_12 <- ungroup(top_n(spooky_wrd_topics_12, 5, abs(log_ratio)))
spooky_wrd_topics_12 <- mutate(spooky_wrd_topics_12, term = reorder(term, log_ratio))
p1 <- ggplot(spooky_wrd_topics_12) +
geom_col(aes(term, log_ratio, fill = log_ratio > 0)) +
theme(legend.position = "none") +
labs(y = "Log ratio of beta in topic 2 / topic 1") +
coord_flip()
spooky_wrd_topics_23 <- filter(spooky_wrd_topics, topic2 > .001 | topic3 > .001)
spooky_wrd_topics_23 <- mutate(spooky_wrd_topics_23, log_ratio = log10(topic3 / topic2))
spooky_wrd_topics_23 <- group_by(spooky_wrd_topics_23, direction = log_ratio > 0)
spooky_wrd_topics_23 <- ungroup(top_n(spooky_wrd_topics_23, 5, abs(log_ratio)))
spooky_wrd_topics_23 <- mutate(spooky_wrd_topics_23, term = reorder(term, log_ratio))
p2 <- ggplot(spooky_wrd_topics_23) +
geom_col(aes(term, log_ratio, fill = log_ratio > 0)) +
theme(legend.position = "none") +
labs(y = "Log ratio of beta in topic 3 / topic 2") +
coord_flip()
spooky_wrd_topics_13 <- filter(spooky_wrd_topics, topic3 > .001 | topic1 > .001)
spooky_wrd_topics_13 <- mutate(spooky_wrd_topics_13, log_ratio = log10(topic3 / topic1))
spooky_wrd_topics_13 <- group_by(spooky_wrd_topics_13, direction = log_ratio > 0)
spooky_wrd_topics_13 <- ungroup(top_n(spooky_wrd_topics_13, 5, abs(log_ratio)))
spooky_wrd_topics_13 <- mutate(spooky_wrd_topics_13, term = reorder(term, log_ratio))
p3 <- ggplot(spooky_wrd_topics_13) +
geom_col(aes(term, log_ratio, fill = log_ratio > 0)) +
theme(legend.position = "none") +
labs(y = "Log ratio of beta in topic 3 / topic 1") +
coord_flip()
layout <- matrix(c(1,2,3), 3, 1, byrow = TRUE)
multiplot(p1, p2, p3, layout = layout)
spooky_wrd_docs <- tidy(spooky_wrd_lda, matrix = "gamma")
spooky_wrd_docs
left_join(spooky_wrd_docs, spooky, by = c("document" = "id"))
aa <- left_join(spooky_wrd_docs, spooky, by = c("document" = "id"))
aa
aa <- select(aa, -text)
head(aa)
aa <- mutate(aa, topic = as.factor(topic))
aa <- top_n(group_by(aa, document), 1, gamma)
head(aa)
spooky_wrd_docs <- tidy(spooky_wrd_lda, matrix = "gamma")
spooky_wrd_docs
spooky_wrd_docs
head(spooyk)
head(spooky)
aa <- left_join(spooky_wrd_docs, spooky, by = c("document" = "id"))
head(aa)
aa
aa <- select(aa, -text)
aa$topic <- as.factor(aa$topic)
aa <- top_n(group_by(aa, document), 1, gamma)
aa
aa <- left_join(spooky_wrd_docs, spooky, by = c("document" = "id"))
aa <- select(aa, -text)
aa$topic <- as.factor(aa$topic)
aa <- ungroup(top_n(group_by(aa, document), 1, gamma))
aa <- left_join(spooky_wrd_docs, spooky, by = c("document" = "id"))
aa <- select(aa, -text)
aa$topic <- as.factor(aa$topic)
aa <- ungroup(top_n(group_by(aa, document), 1, gamma))
aa <- ungroup(count(group_by(aa, author, topic)))
aa
sum(aa[1:10, 3])
table(spooky$author)
sum(aa[11:21, 3])
sum(aa[11:20, 3])
sum(aa[21:30, 3])
author_topics <- left_join(spooky_wrd_docs, spooky, by = c("document" = "id"))
author_topics <- author_topics[, -"text"]
author_topics <- author_topics[, -text]
head(author_topics)
author_topics <- select(author_topics, -text)
author_topics$topic <- as.factor(author_topics$topic)
spooky <- read.csv('../data/spooky.csv', as.is = TRUE)
sum(is.na(spooky))
spooky$author <- as.factor(spooky$author)
# Make a table with one word per row and remove `stop words` (i.e. the common words).
spooky_wrd <- unnest_tokens(spooky, word, text)
library(ggplot2)
library(dplyr)
library(tibble)
library(tidyr)
library(stringr)
library(tidytext)
library(topicmodels)
library(wordcloud)
library(ggridges)
source("../libs/multiplot.R")
# Make a table with one word per row and remove `stop words` (i.e. the common words).
spooky_wrd <- unnest_tokens(spooky, word, text)
spooky_wrd <- anti_join(spooky_wrd, stop_words, by = "word")
nrc_pos <- filter(get_sentiments('nrc'), sentiment == "positive")
nrc_pos
positive <- inner_join(spooky_wrd, nrc_pos, by = "word")
head(positive)
count(positive, word, sort = TRUE)
head(positive)
?group_by
count(group_by(positive, word, author))
count(positive, word, author)
pos_words     <- count(group_by(positive, word, author))
pos_words_all <- count(group_by(positive, word))
pos_words <- left_join(pos_words, pos_words_all, by = "word")
pos_words <- arrange(pos_words, desc(n.y))
pos_words <- ungroup(head(pos_words, 81))
ggplot(pos_words) +
geom_col(aes(reorder(word, n.y, FUN = min), n.x, fill = author)) +
xlab(NULL) +
coord_flip() +
facet_wrap(~ author) +
theme(legend.position = "none")
ggplot(pos_words) +
geom_col(aes(reorder(word, n.y, FUN = min), n.x, fill = author)) +
xlab(NULL) +
coord_flip() +
facet_wrap(~ author) +
theme(legend.position = "none")
pos_words     <- count(positive, word, author)
pos_words_all <- count(positive, word)
pos_words <- left_join(pos_words, pos_words_all, by = "word")
pos_words <- arrange(pos_words, desc(n.y))
pos_words <- ungroup(head(pos_words, 81))
ggplot(pos_words) +
geom_col(aes(reorder(word, n.y, FUN = min), n.x, fill = author)) +
xlab(NULL) +
coord_flip() +
facet_wrap(~ author) +
theme(legend.position = "none")
count(group_by(spooky_wrd, word, author))
count(spooky_wrd, word, author)
pos_words     <- count(group_by(positive, word, author))
pos_words_all <- count(group_by(positive, word))
pos_words <- left_join(pos_words, pos_words_all, by = "word")
pos_words
pos_words <- arrange(pos_words, desc(n.y))
pos_words
pos_words     <- count(positive, word, author)
pos_words_all <- count(positive, word)
pos_words <- left_join(pos_words, pos_words_all, by = "word")
pos_words <- arrange(pos_words, desc(n.y))
pos_words
pos_words     <- count(group_by(positive, word, author))
pos_words_all <- count(group_by(positive, word))
pos_words2     <- count(positive, word, author)
pos_words_all2 <- count(positive, word)
pos_words
pos_words2
pos_words_all
pos_words_all2
pos_words <- left_join(pos_words, pos_words_all, by = "word")
pos_words2 <- left_join(pos_words2, pos_words_all2, by = "word")
pos_words2
pos_words
pos_words2 <- arrange(pos_words2, desc(n.y))
pos_words <- arrange(pos_words, desc(n.y))
pos_words
pos_words2
getwd()
spooky <- read.csv('../data/spooky.csv', as.is = TRUE)
library(ggplot2)
library(dplyr)
library(tibble)
library(tidyr)
library(stringr)
library(tidytext)
library(topicmodels)
library(wordcloud)
library(ggridges)
spooky_wrd <- unnest_tokens(spooky, word, text)
spooky_wrd <- anti_join(spooky_wrd, stop_words, by = "word")
# Words is a list of words, and freqs their frequencies
words <- count(group_by(spooky_wrd, word))$word
freqs <- count(group_by(spooky_wrd, word))$n
head(sort(freqs, decreasing = TRUE))
png("../figs/Wordcloud_all.png")
wordcloud(words, freqs, max.words = 50, color = c("purple4", "red4", "black"))
dev.off()
png("../figs/Wordcloud_all.png")
wordcloud(words, freqs, max.words = 50, color = c("purple4", "red4", "black"))
dev.off()
getwd()
wordcloud(words, freqs, max.words = 50, color = c("purple4", "red4", "black"))
stop_words
packages.used <- c("ggplot2", "dplyr", "tibble", "tidyr",  "stringr", "tidytext", "topicmodels", "wordcloud", "ggridges")
# check packages that need to be installed.
packages.needed <- setdiff(packages.used, intersect(installed.packages()[,1], packages.used))
# install additional packages
if(length(packages.needed) > 0) {
install.packages(packages.needed, dependencies = TRUE, repos = 'http://cran.us.r-project.org')
}
library(ggplot2)
library(dplyr)
library(tibble)
library(tidyr)
library(stringr)
library(tidytext)
library(topicmodels)
library(wordcloud)
library(ggridges)
source("../libs/multiplot.R")
spooky <- read.csv('../data/spooky.csv', as.is = TRUE)
head(spooky)
summary(spooky)
sum(is.na(spooky))
spooky$author <- as.factor(spooky$author)
# Make a table with one word per row and remove `stop words` (i.e. the common words).
spooky_wrd <- unnest_tokens(spooky, word, text)
spooky_wrd <- anti_join(spooky_wrd, stop_words, by = "word")
# Words is a list of words, and freqs their frequencies
words <- count(group_by(spooky_wrd, word))$word
freqs <- count(group_by(spooky_wrd, word))$n
head(sort(freqs, decreasing = TRUE))
png("../figs/Wordcloud_all.png")
wordcloud(words, freqs, max.words = 50, color = c("purple4", "red4", "black"))
dev.off()
# Counts number of times each author used each word.
author_words <- count(group_by(spooky_wrd, word, author))
# Counts number of times each word was used.
all_words    <- rename(count(group_by(spooky_wrd, word)), all = n)
author_words <- left_join(author_words, all_words, by = "word")
author_words <- arrange(author_words, desc(all))
author_words <- ungroup(head(author_words, 81))
ggplot(author_words) +
geom_col(aes(reorder(word, all, FUN = min), n, fill = author)) +
xlab(NULL) +
coord_flip() +
facet_wrap(~ author) +
theme(legend.position = "none")
p1 <- ggplot(spooky) +
geom_bar(aes(author, fill = author)) +
theme(legend.position = "none")
spooky$sen_length <- str_length(spooky$text)
head(spooky$sen_length)
p2 <- ggplot(spooky) +
geom_density_ridges(aes(sen_length, author, fill = author)) +
scale_x_log10() +
theme(legend.position = "none") +
labs(x = "Sentence length [# characters]")
spooky_wrd$word_length <- str_length(spooky_wrd$word)
head(spooky_wrd$word_length)
p3 <- ggplot(spooky_wrd) +
geom_density(aes(word_length, fill = author), bw = 0.05, alpha = 0.3) +
scale_x_log10() +
theme(legend.position = "none") +
labs(x = "Word length [# characters]")
layout <- matrix(c(1, 2, 1, 3), 2, 2, byrow = TRUE)
multiplot(p1, p2, p3, layout = layout)
frequency <- count(spooky_wrd, author, word)
tf_idf    <- bind_tf_idf(frequency, word, author, n)
head(tf_idf)
tail(tf_idf)
tf_idf    <- arrange(tf_idf, desc(tf_idf))
tf_idf    <- mutate(tf_idf, word = factor(word, levels = rev(unique(word))))
# Grab the top thirty tf_idf scores in all the words
tf_idf_30 <- top_n(tf_idf, 30, tf_idf)
ggplot(tf_idf_30) +
geom_col(aes(word, tf_idf, fill = author)) +
labs(x = NULL, y = "TF-IDF values") +
theme(legend.position = "top", axis.text.x  = element_text(angle=45, hjust=1, vjust=0.9))
# Grab the top twenty tf_idf scores in all the words for each author
tf_idf <- ungroup(top_n(group_by(tf_idf, author), 20, tf_idf))
ggplot(tf_idf) +
geom_col(aes(word, tf_idf, fill = author)) +
labs(x = NULL, y = "tf-idf") +
theme(legend.position = "none") +
facet_wrap(~ author, ncol = 3, scales = "free") +
coord_flip() +
labs(y = "TF-IDF values")
# Keep words that have been classified within the NRC lexicon.
get_sentiments('nrc')
sentiments <- inner_join(spooky_wrd, get_sentiments('nrc'), by = "word")
count(sentiments, sentiment)
count(sentiments, author, sentiment)
ggplot(count(sentiments, sentiment)) +
geom_col(aes(sentiment, n, fill = sentiment))
ggplot(count(sentiments, author, sentiment)) +
geom_col(aes(sentiment, n, fill = sentiment)) +
facet_wrap(~ author) +
coord_flip() +
theme(legend.position = "none")
nrc_pos <- filter(get_sentiments('nrc'), sentiment == "positive")
nrc_pos
positive <- inner_join(spooky_wrd, nrc_pos, by = "word")
head(positive)
count(positive, word, sort = TRUE)
pos_words     <- count(group_by(positive, word, author))
pos_words_all <- count(group_by(positive, word))
pos_words <- left_join(pos_words, pos_words_all, by = "word")
pos_words <- arrange(pos_words, desc(n.y))
pos_words <- ungroup(head(pos_words, 81))
# Note the above is the same as
# pos_words <- pos_words  %>%
#                left_join(pos_words_all, by = "word") %>%
#                arrange(desc(n.y)) %>%
#                head(81) %>%
#                ungroup()
ggplot(pos_words) +
geom_col(aes(reorder(word, n.y, FUN = min), n.x, fill = author)) +
xlab(NULL) +
coord_flip() +
facet_wrap(~ author) +
theme(legend.position = "none")
# Counts how many times each word appears in each sentence
sent_wrd_freqs <- count(spooky_wrd, id, word)
head(sent_wrd_freqs)
# Creates a DTM matrix
spooky_wrd_tm <- cast_dtm(sent_wrd_freqs, id, word, n)
spooky_wrd_tm
length(unique(spooky_wrd$id))
length(unique(spooky_wrd$word))
spooky_wrd_lda    <- LDA(spooky_wrd_tm, k = 12, control = list(seed = 1234))
spooky_wrd_topics <- tidy(spooky_wrd_lda, matrix = "beta")
spooky_wrd_topics
# Grab the top five words for each topic.
spooky_wrd_topics_5 <- ungroup(top_n(group_by(spooky_wrd_topics, topic), 5, beta))
spooky_wrd_topics_5 <- arrange(spooky_wrd_topics_5, topic, -beta)
spooky_wrd_topics_5 <- mutate(spooky_wrd_topics_5, term = reorder(term, beta))
ggplot(spooky_wrd_topics_5) +
geom_col(aes(term, beta, fill = factor(topic)), show.legend = FALSE) +
facet_wrap(~ topic, scales = "free", ncol = 4) +
coord_flip()
spooky_wrd_topics <- mutate(spooky_wrd_topics, topic = paste0("topic", topic))
spooky_wrd_topics <- spread(spooky_wrd_topics, topic, beta)
spooky_wrd_topics_12 <- filter(spooky_wrd_topics, topic2 > .001 | topic3 > .001)
spooky_wrd_topics_12 <- mutate(spooky_wrd_topics_12, log_ratio = log10(topic2 / topic1))
spooky_wrd_topics_12 <- group_by(spooky_wrd_topics_12, direction = log_ratio > 0)
spooky_wrd_topics_12 <- ungroup(top_n(spooky_wrd_topics_12, 5, abs(log_ratio)))
spooky_wrd_topics_12 <- mutate(spooky_wrd_topics_12, term = reorder(term, log_ratio))
p1 <- ggplot(spooky_wrd_topics_12) +
geom_col(aes(term, log_ratio, fill = log_ratio > 0)) +
theme(legend.position = "none") +
labs(y = "Log ratio of beta in topic 2 / topic 1") +
coord_flip()
spooky_wrd_topics_23 <- filter(spooky_wrd_topics, topic2 > .001 | topic3 > .001)
spooky_wrd_topics_23 <- mutate(spooky_wrd_topics_23, log_ratio = log10(topic3 / topic2))
spooky_wrd_topics_23 <- group_by(spooky_wrd_topics_23, direction = log_ratio > 0)
spooky_wrd_topics_23 <- ungroup(top_n(spooky_wrd_topics_23, 5, abs(log_ratio)))
spooky_wrd_topics_23 <- mutate(spooky_wrd_topics_23, term = reorder(term, log_ratio))
p2 <- ggplot(spooky_wrd_topics_23) +
geom_col(aes(term, log_ratio, fill = log_ratio > 0)) +
theme(legend.position = "none") +
labs(y = "Log ratio of beta in topic 3 / topic 2") +
coord_flip()
spooky_wrd_topics_13 <- filter(spooky_wrd_topics, topic3 > .001 | topic1 > .001)
spooky_wrd_topics_13 <- mutate(spooky_wrd_topics_13, log_ratio = log10(topic3 / topic1))
spooky_wrd_topics_13 <- group_by(spooky_wrd_topics_13, direction = log_ratio > 0)
spooky_wrd_topics_13 <- ungroup(top_n(spooky_wrd_topics_13, 5, abs(log_ratio)))
spooky_wrd_topics_13 <- mutate(spooky_wrd_topics_13, term = reorder(term, log_ratio))
p3 <- ggplot(spooky_wrd_topics_13) +
geom_col(aes(term, log_ratio, fill = log_ratio > 0)) +
theme(legend.position = "none") +
labs(y = "Log ratio of beta in topic 3 / topic 1") +
coord_flip()
layout <- matrix(c(1,2,3), 3, 1, byrow = TRUE)
multiplot(p1, p2, p3, layout = layout)
spooky_wrd_docs <- tidy(spooky_wrd_lda, matrix = "gamma")
spooky_wrd_docs
author_topics <- left_join(spooky_wrd_docs, spooky, by = c("document" = "id"))
author_topics <- select(author_topics, -text)
author_topics$topic <- as.factor(author_topics$topic)
# Chooses the top topic per sentence
author_topics <- ungroup(top_n(group_by(author_topics, document), 1, gamma))
# Counts the number of sentences represented by each topic per author
author_topics <- ungroup(count(group_by(author_topics, author, topic)))
author_topics
?kmeans
?prcomp
