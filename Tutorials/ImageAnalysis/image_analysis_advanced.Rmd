---
title: "Advanced Image Analysis"
output: html_notebook
---

In this notebook, we discuss more advanced image analysis techniques. Unfortunately,
a number of these techniques are not available in R packages, but only in python packages.
We will mostly be using a R to Python bridge to use these features, provided by the reticulate
package. To reproduce the code below, you will need to install the [reticulate](https://rstudio.github.io/reticulate/articles/introduction.html)
package, but also make sure that the Python environment being called by the reticulate
package contains the requisite dependencies.

If you do not have an existing scientific python installation consider installing
[Anaconda Python](https://www.anaconda.com/download/). I recommend the Python 3.6
version.

We will also be using EBImage on the R side to be able to display the images in R
when we get them from Python.

```{r}
library(reticulate)

# If you are using anaconda, point reticulate to the correct conda environment
# use_condaenv('your-environment')

# for some reason I need to import cv2 and tensorflow before EBImage
# or everything breaks.
cv2 <- reticulate::import('cv2')

library(EBImage)
```

We will use the Python OpenCV package. This package implements a wide array of advanced
image analysis methods that may be useful in extracting features.

# Introduction to OpenCV

The OpenCV package is called `cv2` in python, and contains functionality to
perform both basic image analysis tasks and more advanced image analysis tasks.
To import the python package in R we will use reticulate's `import` function,
which is similar to R's `library` function, except that we must name the result.

```{r}
# This imports the cv2 package.
cv2 <- reticulate::import('cv2')
```

We can read an image from CV2 as an array of pixel values. Note that cv2 imread
reads in the image as values from 0 to 255, instead of values from 0 to 1, so
we are required to divide by 255 to obtain the floating point image values
we are used to.
```{r}
img <- cv2$imread('pet1.jpg') / 255
img_leaf <- cv2$imread('leaf.png') / 255
```

We can convert this to an `EBImage::Image` to display in R. However, we must take
care of two things: the image is read in with integer values 0-255 instead of
floating point values 0-1, and the rows and columns are permuted due to different
conventions in storing two-dimensional arrays in R and Python. Nonetheless, we can
convert the data into an image and display it in R:

```{r}
img_r <- EBImage::Image(aperm(img, c(2, 1, 3)), colormode = 'Color')
plot(img_r)
```

Here, we have made two modifications to the image: first, we have used
the `aperm` function to permute the first and second dimensions of the `img`
array, to swap the rows and the columns. Secondly, we have divided by 255
to convert back to a value between 0 and 1.

Since we will use this more, let us create a function to convert a Python Image
to a R image.

```{r}
to_ebimage <- function(img) {
    EBImage::Image(aperm(img, c(2, 1, 3)), colormode = 'Color')
}
```

# Image analysis and features

A step common to almost all image analysis strategies is to extract so-called features
from an image. Indeed, it is often difficult to reason about the pixels of an image
directly, as they interact in complex fashions that is difficult to capture (in fact,
only one type of model today reasons from pixel values directly: neural networks).
Instead, we will attempt to extract some numbers from the image which hopefully correspond
to higher-order features of the image, and train our classifiers on the extracted values
(often called features).

OpenCV implements a wide variety of features. We will focus on a number of estimators
based on HOG (Histogram of Oriented Gradients) ideas. These estimators attempt to
extract the orientation of edges in the image, which we believe are one of the main
features of images.

## Image derivatives

Most of the algorithms described today will make use of image derivatives, which
attempt to quantify the change in the image. OpenCV has built in functions to
apply these derivatives. The most common derivatives are the Laplacian and the
Sobel derivatives.

```{r}
laplacian <- cv2$Laplacian(img, cv2$CV_64F)
plot(to_ebimage(laplacian))
```

```{r}
sobel_x <- cv2$Sobel(img, cv2$CV_64F, 1L, 0L)
sobel_y <- cv2$Sobel(img, cv2$CV_64F, 0L, 1L)
plot(to_ebimage(sobel_x))
plot(to_ebimage(sobel_y))
```

These derivatives measure the changes in the image, and are thus often
able to detect edges. The next two procedures will attempt to extract
useful information from such as directions from the edges and build
lower-dimensional features.

## HOG (Histogram of oriented gradients)

The histogram of oriented gradients attempts to extract gradient information
(direction and magnitude) over many blocks of the image.

```{r}
# We create a HOG object
# The only parameter of real importance here is winSize,
# but we are required to pass in at least this many parameters
# so that OpenCV can figure out which function we want to call.

winSize <- tuple(64L,64L)
blockSize <- tuple(16L,16L)
blockStride <- tuple(8L,8L)
cellSize <- tuple(8L,8L)
nbins = 9L

hog = cv2$HOGDescriptor(winSize,blockSize,blockStride,cellSize,nbins)
```

Most algorithms only work over images of fixed sizes. Let us resize the image
and compute the hog descriptor on the image. We note that the HOG descriptor
only accepts inputs in the 0-255 uint8 format, and so we need to specify
this as our input type.
```{r}
img_resized <- cv2$resize(img, dsize=tuple(64L, 64L))
hog_values <- hog$compute(np_array(img_resized * 255, dtype='uint8'))
```

We thus compute a feature vector of length 1764 (in this case) which corresponds
to the HOG features of our image.

## SIFT (Scale-invariant feature transformation)

SIFT is one of the most popular feature extraction tools. It attempts to detect keypoints
at different scales of the image by using a combination of the gradient ideas and the
HOG ideas. OpenCV contains one of the better implementations of SIFT, along with
tools to visualize the SIFT features.

However, note that SIFT can only operate on grayscale images, we thus need to convert
our images to grayscale first. We can use the EBImage tools we have seen in the previous
tutorial, or use the native OpenCV tools.

If we use the native OpenCV tools, we must again be careful to coerce the arrays
to the correct data type (in this case, single precision float).

```{r}
img_gray <- cv2$cvtColor(np_array(img, dtype='float32'), cv2$COLOR_BGR2GRAY)
```

Let us now create a SIFT feature extractor, get the features and draw them
on a given image. Again, the SIFT feature extractor only works on uint8 images,
and we thus need to convert our image back to the specified format.

```{r}
sift <- cv2$xfeatures2d$SIFT_create()
img_gray_uint8 <- np_array(img_gray * 255, dtype='uint8')
keypoints <- sift$detect(img_gray_uint8, NULL)
```

We can draw the keypoints and display the image with all the keypoints.
```{r}
img_keypoints <- cv2$drawKeypoints(img_gray_uint8,
                                   keypoints, NULL,
                                   flags=cv2$DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)

plot(to_ebimage(img_keypoints / 255))
```

SIFT summarises each keypoint with a vector of length 128. We can again use OpenCV
to compute these values. Alternatively, we can do both in one go by using
`sift$detectAndCompute`.
```{r}
keypoints_and_values <- sift$compute(img_gray_uint8, keypoints)
values <- keypoints_and_values[[2]]
dim(values)
```

We have thus obtained vectors describing every keypoint. However, the number
of keypoints varies from images to images, hence any classifier we use on these
features must handle that. Common choices include Bag-of-Words (or Bag-of-Feature)
and Topic models. On the other hand, an advantage of sift is that there is no
need to resize the images as the estimator naturally deals with images of
different sizes.

## Dense SIFT

An alternative to the problem of a variable number of features above is to compute
the so-called dense sift, where we compute the sift features at a number of equally
spaced keypoints. For example, for a 128x128 image, we may attempt to compute features
at a grid of equally sized keypoints (e.g. every 16 pixels). This creates a grid
of 64 keypoints.

```{r}
keypoints_dense <- apply(as.matrix(expand.grid(1:8, 1:8)), 1, function (x) {
  cv2$KeyPoint((x[1] - 0.5) * 16, (x[2] - 0.5) * 16, 16)
})
```

```{r}
img_gray_resized <- cv2$cvtColor(np_array(img_resized, dtype='float32'),
                                 cv2$COLOR_BGR2GRAY)

img_gray_resized_uint8 <- np_array(img_gray_resized * 255, dtype='uint8')

img_keypoints_dense <- cv2$drawKeypoints(
  img_gray_resized_uint8, keypoints_dense, NULL,
  flags=cv2$DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)

plot(to_ebimage(img_keypoints_dense / 255))
```

We may then compute the SIFT features at these equally spaced keypoints. This in
effect extract features that are very similar to the HOG features.

```{r}
res_dense <- sift$compute(img_gray_resized_uint8, keypoints_dense)
values_dense <- res_dense[[2]]
dim(values_dense)
```

We see that we obtain an array of 64 features of dimension 128. As we chose the number
of points, we always have the same number of features and can thus use standard classifiers.
On the other hand, this method requires us to systematically resize the image to a single
consistent size.

## Other feature extractors.

OpenCV implements numerous experimental feature extractors, which may be found
[here](https://docs.opencv.org/3.4.0/d7/d7a/group__xfeatures2d__experiment.html).
They vary in the goal and type of feature produced, but you may wish to explore
some of them for your project.

# Neural network features

Neural networks are a vast class of methods that are able to train classifiers
from the data. However, we are also able to make use of pre-trained neural networks
as feature extractors, avoiding the expensive training process (usually several days).
This method is often called [transfer learning](https://codelabs.developers.google.com/codelabs/tensorflow-for-poets/#0)
and we will try extract some features from a small network called mobilenet today.

As with OpenCV, tensorflow is mainly a Python library, so the R library is simply a bridge
to the python library that you must have on your computer. You can folllow the
instructions on the R tensorflow [website](https://tensorflow.rstudio.com/tensorflow/)
to install tensorflow.
```{r}
# In theory, we should just be able to use the line below,
# but I have some compatibility issues.

# library(tensorflow)
tf <- import('tensorflow')
```

```{r}
# Tensorflow uses global state, this resets it if things get
# mucked up.
tf$reset_default_graph()
```

We load a pretrained network called MobileNet. It has the advantage of
being small and fast but also reasonably accurate. It was trained on a large
dataset called [ImageNet](http://www.image-net.org/), and it is believed that
such a procedure creates an estimator that is able to capture general properties
of images.
```{r}
model_path <- 'mobilenet_v1_0.50_224/quantized_graph.pb'

data <- with(tf$gfile$FastGFile(model_path, 'rb') %as% f, {
  f$read()
})

graph_def <- tf$GraphDef()
graph_def$ParseFromString(data)
```

Tensorflow represents networks as large graphs of operations. We need to get a
reference to the "first" node, where the data is fed in, and a reference to
the last node, wher features are output.
```{r}
graph_elems <- tf$import_graph_def(
  graph_def,
  name='',
  return_elements=c('input:0',
                    'MobilenetV1/Predictions/Reshape:0'))

graph_input <- graph_elems[[1]]
graph_output <- graph_elems[[2]]
```

```{r}
graph_input
graph_output
```

We see that the first node should be an image of size 224 x 224, and the output
is a vector of length 1001.

Let us get our image into the right format, and run it through the network.
Note that this network was designed to have the image scaled from -1 to 1,
so we need to do some rescaling of the image again.
```{r}
img_resized <- cv2$resize(img, tuple(224L, 224L))
img_nnet <- (img_resized - 0.5) * 2
```

To run operations in tensorflow we need access to a session. You only need to create
a single one (in fact, you should avoid creating more than one).
```{r}
sess <- tf$Session()
```

We now convert our image to the required shape and ask tensorflow to run the required
operations. This produces a feature vector of length 1001 that we can then use to
perform classification.
```{r}
input_img <- np_array(img_nnet, dtype='float32')
input_img <- input_img$reshape(1L, 224L, 224L, 3L)

features <- sess$run(
  graph_output,
  feed_dict=dict(graph_input=input_img))

features <- as.vector(features)
```

Unfortunately, although such features are often able to produce good estimators,
they are usually not easily interpretable. We have plotted the feature values
below, but they do not necessarily give any insight into the image. Additionally,
further methods are required to perform detection if it is desired.
```{r}
plot(features)
```